{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea63feb3",
   "metadata": {},
   "source": [
    "Derek Yadgaroff\n",
    "\n",
    "Assignment 3, Exercise 2\n",
    "\n",
    "21VT-2DV516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2664b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0c98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab5b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "rng = np.random.default_rng(3)\n",
    "permutation = rng.permutation(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4166bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize Data\n",
    "X_randomized = X[permutation]\n",
    "y_randomized = y[permutation]\n",
    "\n",
    "# Normalize Data\n",
    "scaler = MinMaxScaler()\n",
    "X_randomized_normalized = scaler.fit_transform(X_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f10dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n",
      "(42000, 784) (42000,)\n"
     ]
    }
   ],
   "source": [
    "# Pick size of dataset to use for gridsearch since it is very large\n",
    "reduction = .6\n",
    "reduced = int(X.shape[0]*reduction)\n",
    "\n",
    "# This is a view, not a copy!\n",
    "X_randomized_normalized_reduced = X_randomized_normalized[:reduced]\n",
    "y_randomized_reduced = y_randomized[:reduced]\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X_randomized_normalized_reduced.shape, y_randomized_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476ea10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_randomized_normalized_reduced,\n",
    "                                                    y_randomized_reduced,\n",
    "                                                    train_size = .7, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e78b9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertuning values of C and gamma \n",
    "# store as pickle for future reference and in case computer shuts down... training takes a while\n",
    "pkl_filename = \"pickle_model_p6_p98167.pkl\"\n",
    "try:\n",
    "    clf = pickle.load(open(pkl_filename, \"rb\"))\n",
    "except (OSError, IOError) as e:\n",
    "    gscv = GridSearchCV(SVC(), {'C':[.01, .1,1,10,100]}, cv = 3, refit=True, n_jobs = -1)\n",
    "    gscv.fit(X_train,y_train)\n",
    "    clf = gscv.best_estimator_\n",
    "    pickle.dump(clf, open(pkl_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5610614",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gscv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-db0aaef22da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train score: {str(round(abs(gscv.best_score_)))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgscv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} : {str(gscv.best_params_[key])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"test accuracy: {accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gscv' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"train score: {str(round(abs(gscv.best_score_)))}\")\n",
    "for key in gscv.best_params_:\n",
    "    print(f\"{key} : {str(gscv.best_params_[key])}\")\n",
    "accuracy = str(round(abs(clf.score(X_test, y_test)),5))\n",
    "print(f\"test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b08592",
   "metadata": {},
   "source": [
    "### Start Over and train with all the data using the C and gamm param chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_randomized_normalized,\n",
    "                                                    y_randomized,\n",
    "                                                    train_size = .8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b92922",
   "metadata": {},
   "source": [
    "#### 1a. Get benchmark of sklearn SVM with param C=10, taken from gridsearch above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertuning values of C and gamma \n",
    "# store as pickle for future reference and in case computer shuts down... training takes a while\n",
    "pkl_filename = \"pickle_model_allsamples.pkl\"\n",
    "try:\n",
    "    clfsklearn = pickle.load(open(pkl_filename, \"rb\"))\n",
    "except (OSError, IOError) as e:\n",
    "    clfsklearn = SVC(C=10)\n",
    "    clfsklearn.fit(X_train,y_train)\n",
    "    pickle.dump(clfsklearn, open(pkl_filename, \"wb\"))\n",
    "Zsklearn = clfsklearn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78028db",
   "metadata": {},
   "source": [
    "#### 1b. Print Num correct, num incorrect, and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771dfcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = np.sum(Z==y_test)\n",
    "num_incorrect = np.sum(Z!=y_test)\n",
    "print(num_correct,num_incorrect, (num_correct/(num_correct+num_incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, prediction in zip(axes, X_test, Zsklearn):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f'Prediction: {prediction}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bdf885",
   "metadata": {},
   "source": [
    "#### 2. Setup One vs All\n",
    "\n",
    "For each class, create a y_test array where labels are either the relevant class or \"ALL\". Now it is a binary classifier for each class, true if it is the correct class or false if it is any other class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c44d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_vs_all = []\n",
    "for u in np.unique(y_train):\n",
    "    arr = np.copy(y_train)\n",
    "    arr[y_train != u] = \"ALL\"\n",
    "    y_one_vs_all.append(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e07c87",
   "metadata": {},
   "source": [
    "#### 3. Train a classifier for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72bf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for y_one in y_one_vs_all:\n",
    "    clf = SVC(C=10)\n",
    "    clf.fit(X_train,y_one)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ccbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store previous cell\n",
    "pkl_filename = \"y_one_vs_all_clfs.pkl\"\n",
    "pickle.dump(clfs, open(pkl_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921aa473",
   "metadata": {},
   "source": [
    "#### 4. Use the trained classifiers to predict\n",
    "- modify the y_test again like we did before to be binary true/false based on the class being predicted\n",
    "- predict the labels from X_test \n",
    "- print the mean score from the 10 classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abede66",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ova = []\n",
    "for index, clfova in enumerate(clfs):\n",
    "    y_test_ova = np.copy(y_test)\n",
    "    y_test_ova[y_test_ova != str(index)] = \"ALL\"\n",
    "    ova_score = clfova.score(X_test,y_test_ova)\n",
    "    scores_ova.append(ova_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269bb5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = \"y_one_vs_all_scores.pkl\"\n",
    "pickle.dump(scores_ova, open(pkl_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_ova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The score for each classifier is {ova_score}\")\n",
    "print(f\"The mean score is {np.mean(ova_score)}\")\n",
    "print(f\"This means that the one-vs-all has outperformed the built in sklearn which had a score of {num_correct/(num_correct+num_incorrect)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7af7bd",
   "metadata": {},
   "source": [
    "Which was the best classifier? If studying the confusion matrix was there any apparent difference between the two methods in terms of misclassifications? Include your findings either as comments in your code, in your Jupyter notebook or as a separate text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a60868e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
